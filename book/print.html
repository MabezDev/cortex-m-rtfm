<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The RTFM book</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="_FontAwesome/css/font-awesome.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        

    </head>
    <body class="light">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { } 
            if (theme === null || theme === undefined) { theme = 'light'; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <ol class="chapter"><li><a href="preface.html"><strong aria-hidden="true">1.</strong> Preface</a></li><li><a href="user/guide.html"><strong aria-hidden="true">2.</strong> User guide</a></li><li><ol class="section"><li><a href="user/basic.html"><strong aria-hidden="true">2.1.</strong> Basic organization</a></li><li><a href="user/events.html"><strong aria-hidden="true">2.2.</strong> Reacting to events</a></li><li><a href="user/state.html"><strong aria-hidden="true">2.3.</strong> Adding state</a></li><li><ol class="section"><li><a href="user/late-resources.html"><strong aria-hidden="true">2.3.1.</strong> Runtime initialized resources</a></li></ol></li><li><a href="user/messages.html"><strong aria-hidden="true">2.4.</strong> Message passing</a></li><li><a href="user/scheduling.html"><strong aria-hidden="true">2.5.</strong> Priority based scheduling</a></li><li><a href="user/sharing.html"><strong aria-hidden="true">2.6.</strong> Resource sharing</a></li><li><a href="user/pools.html"><strong aria-hidden="true">2.7.</strong> Object pools</a></li><li><a href="user/periodic.html"><strong aria-hidden="true">2.8.</strong> Periodic tasks</a></li></ol></li><li><a href="internals.html"><strong aria-hidden="true">3.</strong> Under the hood</a></li><li><ol class="section"><li><a href="internals/scheduler.html"><strong aria-hidden="true">3.1.</strong> The scheduler</a></li><li><a href="internals/claim.html"><strong aria-hidden="true">3.2.</strong> claim</a></li><li><a href="internals/messages.html"><strong aria-hidden="true">3.3.</strong> Message passing</a></li><li><ol class="section"><li><a href="internals/dispatcher.html"><strong aria-hidden="true">3.3.1.</strong> Dispatching tasks</a></li><li><a href="internals/schedule-now.html"><strong aria-hidden="true">3.3.2.</strong> schedule_now</a></li><li><a href="internals/capacity.html"><strong aria-hidden="true">3.3.3.</strong> Capacity</a></li></ol></li><li><a href="internals/tq.html"><strong aria-hidden="true">3.4.</strong> The timer queue</a></li><li><ol class="section"><li><a href="internals/tq/schedule-after.html"><strong aria-hidden="true">3.4.1.</strong> schedule_after</a></li><li><a href="internals/tq/handler.html"><strong aria-hidden="true">3.4.2.</strong> The timer queue handler</a></li></ol></li><li><a href="internals/ceilings.html"><strong aria-hidden="true">3.5.</strong> Ceiling analysis</a></li></ol></li></ol>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light <span class="default">(default)</span></button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">The RTFM book</h1> 

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <a class="header" href="print.html#preface" id="preface"><h1>Preface</h1></a>
<p>This book documents the Real Time For the Masses framework from the point of view of a user and from
the point of view of an implementer.</p>
<a class="header" href="print.html#user-guide" id="user-guide"><h1>User guide</h1></a>
<p>This section introduces the Real Time For the Masses framework to a new user through various
examples presented in order of increasing complexity.</p>
<a class="header" href="print.html#basic-organization" id="basic-organization"><h1>Basic organization</h1></a>
<p>This section presents the structure of a basic RTFM program.</p>
<p>Below is shown a minimal RTFM program.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)] // required to import the `app!` macro
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;
extern crate panic_abort; // panicking behavior
extern crate stm32f103xx; // device crate

use cortex_m_rtfm::app;

app! {
    device: stm32f103xx,
}

fn init(ctxt: init::Context) -&gt; init::LateResources {
    // Cortex-M peripherals
    let core = ctxt.core;

    // device peripherals
    let device = ctxt.device;

    init::LateResources {} // more on this later
}

fn idle(ctxt: idle::Context) -&gt; ! {
    loop {
        // do stuff here
    }
}
#}</code></pre></pre>
<p>All RTFM applications include an invocation of the <code>app!</code> macro; this macro is the <em>specification</em>
of the application. At the very least you'll have to declare the device you are using in this macro.
I'll be using the STM32F103RB microcontroller throughout this book so I'll use <code>stm32f103xx</code> for the
value of the <code>device</code> field. The value of this field is the <em>path</em> to a device crate, a crate
generated using <code>svd2rust</code>.</p>
<p>The <code>app!</code> macro generates the entry point of the program: the <code>main</code> function. So, instead of a
<code>main</code> function you have to provide <em>two</em> functions: <code>idle</code> and <code>init</code>. The signatures of those two
functions are shown in the minimal example. The <code>main</code> function generated by the <code>app!</code> macro will
call the <code>init</code> function <em>with interrupts disabled</em> first and then it will call the never ending
<code>idle</code> function.</p>
<p>The arguments of the <code>init</code> and <code>idle</code> are these <code>Context</code> values. These <code>Context</code> structs have the
following fields:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// generated by the `app!` macro
mod init {
    struct Context {
        // pretty similar to `cortex_m::Peripherals`, minus some fields
        pub core: CorePeripherals,
        pub device: stm32f103xx::Peripherals,
        pub resources: Resources,
        pub tasks: Tasks,
        pub threshold: Threshold,
    }

    // ..
}

// generated by the `app!` macro
mod idle {
    pub struct Context {
        pub resources: Resources,
        pub threshold: Threshold,
    }

    // ..
}
#}</code></pre></pre>
<p>That covers the structure of a minimal RTFM application. RTFM applications are usually structured as
a set of <em>tasks</em>. In the next section we'll see how create tasks.</p>
<a class="header" href="print.html#reacting-to-events" id="reacting-to-events"><h1>Reacting to events</h1></a>
<p>RTFM main use case is building reactive systems: systems that respond to external stimuli. In RTFM,
tasks are the main mechanism to respond to <em>events</em>, or interrupt sources.</p>
<p>Below is shown an RTFM program with a single <em>event</em> task:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;

use cortex_m_rtfm::app;

app! {
    device: stm32f103xx,

    tasks: {
        exti0: {
            interrupt: EXTI0, // this interrupt corresponds to the user pressing a button
        },
    },
}

// omitted: init and idle

// the body of the `exti0` task
fn exti0(ctxt: exti0::Context) {
    // executed whenever a certain button is pressed

    println!(&quot;User pressed a button&quot;);
}
#}</code></pre></pre>
<p>Here we have a task named <code>exti0</code> bound to the <code>EXTI0</code> interrupt source. The <code>exti0</code> task starts,
i.e. the <code>exti0</code> function is called, whenever the <code>EXTI0</code> interrupt fires. Interrupts are device
specific and come from the device crate, <code>stm32f103xx</code>. In this case the interrupt <code>EXTI0</code> is
triggered by a change in the logic level of a digital input pin. In this example said pin is
connected to a button; thus pressing the button triggers the <code>exti0</code> task.</p>
<p>Each task has access to a <code>Context</code>. The fields of this <code>Context</code> struct are:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// generated by the `app!` macro
mod exti0 {
    pub struct Context {
        pub input: (),
        pub resources: Resources,
        pub tasks: Tasks,
        pub threshold: Threshold,
    }

    // ..
}
#}</code></pre></pre>
<p>Event tasks map directly to device specific interrupts. The RTFM runtime will take care of both
<em>unmasking</em> those interrupts and setting their priorities in the NVIC <em>after</em> <code>init</code> ends and
<em>before</em> <code>idle</code> starts. Note that in most cases is necessary to also enable the interrupt in the
device specific peripheral to get the interrupt source to fire the event task.</p>
<p>The other consequence of tasks being interrupts is that tasks won't start until after <code>init</code>
ends -- because interrupts are disabled during <code>init</code>. That is if an event occurs during <code>init</code> the
corresponding task will be set as <em>pending</em> but it won't start until <em>after</em> <code>init</code> ends.</p>
<a class="header" href="print.html#adding-state" id="adding-state"><h1>Adding state</h1></a>
<p>Tasks are stateless by default; state can be added by assigning them <em>resources</em>. Resources are
<code>static</code> variables that can be assigned to tasks. If a resource is assigned to a single task then
it's <em>owned</em> by that task and the task has exclusive access to the resource. A resource can also be
<em>shared</em> by two or more tasks; when shared a resource must be <code>claim</code>ed (which may involve a lock)
before its data can be accessed -- this prevents data races. In RTFM it's preferred to use message
passing (more on that later) instead of sharing state.</p>
<p>The following example shows how to assign a resource to a task to preserve state across the
different invocations of the task.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;

use cortex_m_rtfm::app;

app! {
    device: stm32f103xx,

    // declare resources
    resources: {
        // number of times the user pressed the button
        static PRESSES: u32 = 0;
    },

    tasks: {
        exti0: {
            interrupt: EXTI0,

            // assign the `PRESSES` resource to the `exti0` task
            resources: [PRESSES],
        },
    },
}

// omitted: `init` and `idle`

fn exti0(ctxt: exti0::Context) {
    let presses: &amp;mut u32 = ctxt.resources.PRESSES;
    *presses += 1;

    println!(&quot;Button pressed {} times!&quot;, *presses);
}
#}</code></pre></pre>
<a class="header" href="print.html#runtime-initialized-resources" id="runtime-initialized-resources"><h1>Runtime initialized resources</h1></a>
<p>Normal <code>static</code> variables in Rust must be assigned an initial value when declared, i.e. at compile
time. Resources don't have this limitation and can be initialized at <em>runtime</em>; these resources are
called &quot;late resources&quot; because they are initialized <em>late</em>. The initial values of late resources
must be returned by the <code>init</code> function.</p>
<p>Consider the following example where we load a cryptographic key from EEPROM and then use the key in
a task.</p>
<blockquote>
<p>TODO newtype to prevent mutability</p>
</blockquote>
<blockquote>
<p>TODO stress that resources can be used to limit scope</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;

use cortex_m_rtfm::app;

app! {
    device: stm32f103xx,

    resources: {
        static KEY: [u8; 256];
    },

    tasks: {
        exti0: {
            interrupt: USART1, // data arrived via the serial interface
            resources: [KEY],
        },
    },
}

fn init(ctxt: init::Context) -&gt; init::LateResources {
    let key = load_from_eeprom();

    init::LateResources {
        KEY: key,
    }
}

// omitted: `idle`

fn usart1(ctxt: usart1::Context) {
    let key: &amp;[u8; 256] = ctxt.resources.KEY;

    // use key to decrypt incoming data
}
#}</code></pre></pre>
<blockquote>
<p>TODO example with mutability e.g. reading the RTC</p>
</blockquote>
<a class="header" href="print.html#message-passing" id="message-passing"><h1>Message passing</h1></a>
<p>So far we have seen tasks as a way to respond to events but events are not the only way to start a
task. A task can schedule another task, optionally passing a message to it.</p>
<p>For example, consider the following application where data is received from the serial interface and
collected into a buffer. <code>\n</code> is used as a frame delimiter; once a frame has been received we want
to process the buffer contents but we don't want to do that in the <code>usart1</code> task because that task
has to keep up with the fast incoming data and it should be short and high priority. So, instead we
<em>send</em> the frame to a <em>lower priority</em> task for further processing; this way we keep the <code>usart1</code>
task responsive.</p>
<blockquote>
<p>TODO graph: show timeline with events</p>
</blockquote>
<blockquote>
<p>TODO use cases section</p>
</blockquote>
<blockquote>
<p>TODO &quot;vs threads&quot; dead lock freedom</p>
</blockquote>
<blockquote>
<p>TODO transition from bare metal programming</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;
extern crate heapless;

use cortex_m_rtfm::app;
use heapless::Vec;
use heapless::consts::*;

app! {
    device: stm32f103xx,

    resources: {
        // 128-byte buffer
        static BUFFER: Vec&lt;u8, U128&gt; = Vec::new();

        // omitted: other resources
    },

    // TODO explain this
    free_interrupts: [EXTI0],

    tasks: {
        // task bound to an interrupt
        usart1: {
            // event = data arrived via the serial interface
            interrupt: USART1,

            // higher priority number = more urgent
            priority: 2,

            // omitted: the exact list of resources assigned to this task

            // tasks that this task can schedule
            schedule_now: [process],
        },

        // task schedulable by other tasks
        process: {
            // the input this task expects
            input: Vec&lt;u8, U128&gt;,

            // if omitted `priority` is assumed to be `1`
            // priority: 1,
        },
    },
}

// omitted: `init` and `idle`

fn usart1(ctxt: usart1::Context) {
    const FRAME_DELIMITER: u8 = b'\n';

    let t = &amp;mut ctxt.threshold;
    let tasks = ctxt.tasks;

    let buffer: &amp;mut Vec&lt;u8, U128&gt; = ctxt.resources.BUFFER;
    let serial: &amp;mut Serial = ctxt.resources.SERIAL;

    // reads a single byte from the serial interface
    let byte = serial.read();

    if byte == FRAME_DELIMITER {
        tasks.process.schedule_now(t, buffer.clone()).unwrap();
        buffer.clear();
    } else {
        if buffer.push(byte).is_err() {
            // omitted: error handling
        }
    }
}

fn process(ctxt: process::Context) {
    let buffer: Vec&lt;u8, U128&gt; = ctxt.input;

    match &amp;buffer[..] {
         &quot;command1&quot; =&gt; /* .. */,
         &quot;command2&quot; =&gt; /* .. */,
         // ..
         _ =&gt; /* .. */,
    }
}
#}</code></pre></pre>
<p>Here we have the <code>exti0</code> task scheduling the <code>process</code> task. The <code>process</code> task expects some input;
the second argument of <code>schedule_now</code> is the expected input. This argument will be sent as a message
to the <code>process</code> task.</p>
<p>Only types that implement the <code>Send</code> trait and have a <code>'static</code> lifetimes can be sent as messages.
This means that messages can't contain references to things like values allocated on the stack of
the task or references to the state of a task.</p>
<p>This constrain forces us to sent a copy of the buffer, which is 128 bytes in size, rather than a
reference, which is 4 bytes in size -- this is rather expensive in terms of memory and execution
time. In a future section we'll see how to make messages much smaller using object pools.</p>
<a class="header" href="print.html#how-is-this-different-from-a-function-call" id="how-is-this-different-from-a-function-call"><h2>How is this different from a function call?</h2></a>
<p>You may be wondering how is message passing different that doing a simple function call as shown
below:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn usart1(ctxt: usart1::Context) {
    const FRAME_DELIMITER: u8 = b'\n';

    let buffer: &amp;mut _ = ctxt.resources.BUFFER;
    let serial: &amp;mut _ = ctxt.resources.SERIAL;

    // reads a single byte from the serial interface
    let byte = serial.read();

    if byte == FRAME_DELIMITER {
        process(buffer);
    } else {
        if buffer.push(byte).is_err() {
            // omitted: error handling
        }
    }
}

fn process(buffer: &amp;Vec&lt;u8, U128&gt;) {
    match &amp;buffer[..] {
         &quot;command1&quot; =&gt; /* .. */,
         &quot;command2&quot; =&gt; /* .. */,
         // ..
         _ =&gt; /* .. */,
    }
}
#}</code></pre></pre>
<p>The function call approach even avoids the expensive copy of the buffer!</p>
<p>The main difference is that a function call will execute <code>process</code> in the <em>same</em> execution context
as the <code>usart1</code> task extending the execution time of the <code>usart1</code> task. Whereas making <code>process</code>
into its own task means that it can be scheduled separately.</p>
<p>In this particular case the <code>process</code> task has lower priority than the <code>usart1</code> task so it won't be
executed until <em>after</em> the <code>usart1</code> task ends. Also, preemption is possible: if a <code>USART1</code> event
occurs while executing the <code>process</code> task the scheduler will prioritize the execution of the
<code>usart1</code> task. The next section has more details about priority based scheduling.</p>
<a class="header" href="print.html#priority-based-scheduling" id="priority-based-scheduling"><h1>Priority based scheduling</h1></a>
<p>We have talked about tasks but we have glossed over how they are scheduled. RTFM uses a priority
based scheduler: tasks with higher priority will preempt lower priority ones. Once a task starts it
runs to completion and will only be suspended if a higher priority task needs to be executed, but
once the higher priority task finishes the lower priority one resumes execution.</p>
<p>Let's illustrate how scheduling works with an example:</p>
<blockquote>
<p>TODO move before message passing</p>
</blockquote>
<blockquote>
<p>TODO simplify <code>schedule_now</code> to <em>not</em> take a second argument if the task <code>input</code> is <code>()</code></p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;

use cortex_m_rtfm::app;

app! {
    device: stm32f103xx,

    init: {
        schedule_now: [a],
    },

    tasks: {
        a: {
            // priority: 1,
            schedule_now: [c],
        },

        b: {
            priority: 2,
        },

        c: {
            priority: 3,
            schedule_now: [b],
        },
    },
}

fn init(ctxt: init::Context) -&gt; init::LateResources {
    let t = &amp;mut ctxt.threshold;

    println!(&quot;IN1&quot;);

    ctxt.tasks.a.schedule_now(t, ());

    println!(&quot;IN2&quot;);

    init::LateResources {}
}

fn idle(ctxt: idle::Context) -&gt; ! {
    println!(&quot;ID&quot;);

    loop {
        // ..
    }
}

fn a(ctxt: a::Context) {
    let t = &amp;mut ctxt.threshold;

    println!(&quot;A1&quot;);

    ctxt.tasks.c.schedule_now(t, ());

    println!(&quot;A2&quot;);
}

fn b(ctxt: b::Context) {
    let t = &amp;mut ctxt.threshold;

    println!(&quot;B&quot;);
}

fn c(ctxt: c::Context) {
    let t = &amp;mut ctxt.threshold;

    println!(&quot;C1&quot;);

    ctxt.tasks.b.schedule_now(t, ());

    println!(&quot;C2&quot;);
}
#}</code></pre></pre>
<p>This program prints:</p>
<pre><code class="language-text">IN1
IN2
A1
C1
C2
B
A2
ID
</code></pre>
<p>The RTFM scheduler is actually hardware based and built on top of the NVIC (Nested Vector Interrupt
Controller) peripheral and the interrupt mechanism of the Cortex-M architecture so tasks can't
run while the interrupts are disabled. Thus tasks scheduled during <code>init</code> won't run until <em>after</em>
<code>init</code> ends regardless of their priority.</p>
<p>The program execution goes like this:</p>
<ul>
<li>
<p><code>init</code> prints &quot;I1&quot;. Then task <code>a</code> is scheduled to run immediately but nothing happens because
interrupts are disabled. <code>init</code> prints &quot;I2&quot;.</p>
</li>
<li>
<p><code>init</code> ends and now tasks can run. Task <code>a</code> preempts <code>idle</code>, which runs after <code>init</code> . <code>idle</code>
is not a task per se because it's never ending, but it has the lowest priority (priority = 0) so
all tasks can preempt it -- all tasks have a priority of 1 or larger.</p>
</li>
<li>
<p>Task <code>a</code> prints &quot;A1&quot; and then schedules task <code>c</code> to run immediately. Because task <code>c</code> has higher
priority than task <code>a</code> it preempts <code>a</code>.</p>
</li>
<li>
<p>Task <code>c</code> starts and print &quot;C1&quot;. Then it schedules task <code>b</code> to run immediately. Because task <code>b</code>
has lower priority than <code>c</code> it gets postponed. Task <code>c</code> prints &quot;C2&quot; and returns.</p>
</li>
<li>
<p>After task <code>c</code> ends task <code>a</code> should be resumed but task <code>b</code> is pending and has higher priority so
task <code>b</code> preempts <code>a</code>. Task <code>b</code> prints &quot;B&quot; and ends.</p>
</li>
<li>
<p>Task <code>a</code> is finally resumed. Task <code>a</code> prints &quot;A2&quot; and returns.</p>
</li>
<li>
<p>After task <code>a</code> ends there's no task pending execution so <code>idle</code> is resumed. <code>idle</code> prints &quot;ID&quot; and
then executes some infinite <code>loop</code>.</p>
</li>
</ul>
<a class="header" href="print.html#resource-sharing" id="resource-sharing"><h1>Resource sharing</h1></a>
<p>We mentioned that in RTFM message passing is preferred over sharing state but sometimes the need of
shared state arises so let's look at an example.</p>
<p>Let's say we have an application with three tasks: one reads data from an accelerometer, the other
reads data from a gyroscope and the last one processes both the accelerometer and gyroscope data.
The first two tasks run periodically at 1 KHz (one thousand times per second); the third task must
start after the other two tasks are done and consumes the data each task produces. Here's one way to
implement such program:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;

use cortex_m_rtfm::app;

struct Acceleration { x: u16, y: u16, z: u16 }

struct AngularRate { x: u16, y: u16, z: u16 }

enum Data {
    Empty,
    Acceleration(Acceleration),
    AngularRate(AngularRate),
}

app! {
    device: stm32f103xx,

    resources: {
        static DATA: Data = Data::Empty;

        // omitted: other resources
    },

    tasks: {
        accelerometer: {
            resources: [ACCELEROMETER, DATA],

            schedule_now: [process],

            // priority: 1,

            // omitted: interrupt source
        },

        gyroscope: {
            resources: [GYROSCOPE, DATA],

            schedule_now: [process],

            // priority: 1,

            // omitted: interrupt source
        },

        process: {
            input: (Acceleration, AngularRate),
        },
    }
}

// omitted: `init`, `idle` and `process`

fn accelerometer(ctxt: accelerometer::Context) {
    let accelerometer = ctxt.resources.ACCELEROMETER;
    let acceleration = accelerometer.read();

    let t = &amp;mut ctxt.threshold;

    let angular_rate = {
        let data: &amp;mut Data = ctxt.resources.DATA.borrow_mut(t);

        match *data {
            // store data
            Data::Empty =&gt; {
                *data = Data::Acceleration(acceleration);
                None
            },

            // overwrite old data
            Data::Acceleration(..) =&gt; {
                *data = Data::Acceleration(acceleration);
                None
            },

            // data pair is ready
            Data::AngularRate(angular_rate) =&gt; {
                *data = Data::Empty;
                Some(angular_rate)
            },
        }
    };

    if let Some(angular_rate) = angular_rate {
        ctxt.tasks.process.schedule_now(t, (acceleration, angular_rate)).unwrap();
    }
}

fn gyroscope(ctxt: accelerometer::Context) {
    let gyroscope = ctxt.resources.GYROSCOPE;
    let angular_rate = gyroscope.read();

    let t = &amp;mut ctxt.threshold;

    let acceleration = {
        let data = ctxt.resources.DATA.borrow_mut(t);

        match *data {
            // store data
            Data::Empty =&gt; {
                *data = Data::AngularRate(angular_rate);
                None
            },

            // data pair is ready
            Data::Acceleration(acceleration) =&gt; {
                *data = Data::Empty;
                Some(acceleration)
            },

            // overwrite old data
            Data::AngularRate(angular_rate) =&gt; {
                *data = Data::AngularRate(angular_rate);
                None
            },
        }
    };

    if let Some(acceleration) = acceleration {
        ctxt.tasks.process.schedule_now(t, (acceleration, angular_rate)).unwrap();
    }
}
#}</code></pre></pre>
<p>In this program the tasks <code>acceloremeter</code> and <code>gyroscope</code> share the <code>DATA</code> resource. This resource
can contain either sensor reading or no data at all. The idea is that either sensor task can start
the <code>process</code> task but only the one that has both readings will do. That's where <code>DATA</code> comes in: if
the <code>accelerometer</code> task happens first it stores its reading into <code>DATA</code>; then when the <code>gyroscope</code>
task occurs it <em>takes</em> the acceloremeter reading from <code>DATA</code>, leaving it empty, and schedules the
<code>process</code> task passing both readings. This setup also supports the other scenario where the
<code>gyroscope</code> task starts before the <code>accelerometer</code> task.</p>
<p>In this particular case both sensor tasks operate at the same priority so preemption is not
possible: if both tasks need to run at about the same time one will run <em>after</em> the other. Without
preemption a data race is not possible so each task can directly borrow (<code>borrow</code> / <code>borrow_mut</code>)
the contents of <code>DATA</code>.</p>
<a class="header" href="print.html#claim" id="claim"><h2><code>claim*</code></h2></a>
<p>If, instead, the sensor tasks had different priorities then the lowest priority task would need to
<em>claim</em> (<code>claim</code> / <code>claim_mut</code>) the resource. <code>claim*</code> creates a critical section and grants access
to the contents of a resource for the span of the critical section. To illustrate let's increase the
priority of <code>accelerometer</code> to 2; <code>gyroscope</code> would then have to access <code>DATA</code> like this:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn gyroscope(ctxt: accelerometer::Context) {
    let gyroscope = ctxt.resources.GYROSCOPE;
    let angular_rate = gyroscope.read();

    let t = &amp;mut ctxt.threshold;

    let acceleration = ctxt.resources.DATA.claim_mut(t, |data: &amp;mut Data, _| {
        // start of critical section
        match *data {
            // store data
            Data::Empty =&gt; {
                *data = Data::AngularRate(angular_rate);
                None
            },

            // data pair is ready
            Data::Acceleration(acceleration) =&gt; {
                *data = Data::Empty;
                Some(acceleration)
            },

            // overwrite old data
            Data::AngularRate(angular_rate) =&gt; {
                *data = Data::AngularRate(angular_rate);
                None
            },
        }
        // end of critical section
    });

    if let Some(acceleration) = acceleration {
        ctxt.tasks.process.schedule_now(t, (acceleration, angular_rate)).unwrap();
    }
}
#}</code></pre></pre>
<a class="header" href="print.html#object-pools" id="object-pools"><h1>Object pools</h1></a>
<p>Let's revisit the message passing example from a few sections ago and make it more efficient using
object pools.</p>
<p><code>heapless</code> provides an object pool abstraction named <code>Pool</code> that uses <em>singleton</em> buffers. A
singleton buffer is statically allocated and represented by a singleton type, a type of which can
only ever exist one instance of. Normally, <code>Pool</code> is <code>unsafe</code> to use because the user has to enforce
the singleton requirement of the buffer. RTFM makes <code>Pool</code> safe by enforcing the singleton property
of buffers. RTFM accomplishes this by turning all uninitialized resources of array type assigned to
<code>init</code> into singleton buffers.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;
extern crate heapless;

use cortex_m_rtfm::app;
use heapless::Vec;
use heapless::consts::*;
use heapless::pool::{Object, Pool, Uninit};

app! {
    device: stm32f103xx,

    resources: {
        static BUFFER: Option&lt;Object&lt;A&gt;&gt; = None;

        // memory for the `POOL`
        static V: [Vec&lt;u8, U128&gt;; 2];
        static POOL: Pool&lt;V&gt;;
        // ..
    },

    init: {
        resources: [V],
    },

    tasks: {
        usart1: {
            interrupt: USART1,

            priority: 2,

            resources: [BUFFER, POOL, SERIAL],
        },

        process: {
            input: Object&lt;V&gt;,

            // priority: 1,

            // `POOL` is shared with the `usart1` task
            resources: [POOL],
        },
    },
}

fn init(ctxt: init::Context) -&gt; init::LateResources {
    // ..

    let v: Uninit&lt;V&gt; = ctxt.resources.V;

    init::LateResources {
        POOL: Pool::new(v),
    }
}

fn usart1(ctxt: usart1::Context) {
    const FRAME_DELIMITER: u8 = b'\n';

    let t = &amp;mut ctxt.threshold;
    let tasks = ctxt.tasks;

    let rbuffer: &amp;mut _ = ctxt.resources.BUFFER;
    let pool: &amp;mut _ = ctxt.resources.POOL.borrow_mut(t);
    let serial: &amp;mut _ = ctxt.resources.SERIAL;

    if rbuffer.is_none() {
        // grab a buffer from the pool
        *rbuffer = Some(pool.alloc().unwrap().init(Vec::new()));
    }

    let buffer = rbuffer.take().unwrap();

    let byte = serial.read();

    if byte == FRAME_DELIMITER {
        // send the buffer to the `process` task
        tasks.process.schedule_now(t, buffer).unwrap();
    } else {
        if buffer.push(byte).is_err() {
            // omitted: error handling
        }

        rbuffer = Some(buffer);
    }
}

fn process(ctxt: process::Context) {
    let buffer = ctxt.input;

    // process buffer
    match &amp;buffer[..] {
         &quot;command1&quot; =&gt; /* .. */,
         &quot;command2&quot; =&gt; /* .. */,
         // ..
         _ =&gt; /* .. */,
    }

    // return the buffer to the pool
    let t = &amp;mut ctxt.threshold;
    ctxt.resources.POOL.claim_mut(t, |pool, _| pool.dealloc(buffer));
}
#}</code></pre></pre>
<p>In this new version we use an object <code>Pool</code> that contains two instances of <code>Vec&lt;u8, U128&gt;</code>. The
<code>usart1</code> task will fill one of the vectors in the <code>Pool</code> with data until it finds the frame
delimiter. Once a frame is completed it will send the frame as an <code>Object</code> to the <code>process</code> task.
Unlike the previous version, the <code>Object</code> value is very cheap to send (move): it's just a single
byte in size. In the next iteration <code>usart1</code> will grab a fresh, different vector from the <code>Pool</code> and
repeat the process.</p>
<p>Once the <code>process</code> task is done processing the buffer it will proceed to return it to the object
<code>Pool</code>.</p>
<a class="header" href="print.html#periodic-tasks" id="periodic-tasks"><h1>Periodic tasks</h1></a>
<p>We have seen the <code>schedule_now</code> method which is used to schedule tasks to run immediately. RTFM
also allows scheduling tasks to run some time in the future via the <code>schedule_in</code> API. In a nutshell
the <code>schedule_in</code> lets you schedule a task to run in a certain number of clock (HCLK) cycles in the
future. The offset that the <code>schedule_in</code> takes as argument is added to the <em>scheduled start time</em>
of the <em>current</em> task to compute the scheduled start time of the newly scheduled task. This lets you
create periodic tasks without accumulating drift.</p>
<p><strong>NOTE</strong> Using the <code>scheduled_in</code> API requires enabling the &quot;timer-queue&quot; feature.</p>
<p>Let's look at an example:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#![feature(proc_macro)]
#![no_std]

#fn main() {
extern crate cortex_m_rtfm;

use cortex_m_rtfm::app;

app! {
    device: stm32f103xx,

    init: {
        schedule_now: [a],
    },

    tasks: {
        a: {
            schedule_after: [a],
        },
    },
}

fn init(ctxt: init::Context) -&gt; init::LateResources {
    let t = &amp;mut ctxt.threshold;

    ctxt.tasks.a.schedule_now(t, ());
}

// number of clock cycles equivalent to 1 second
const S: u32 = 8_000_000;

fn a(ctxt: a::Context) {
    // `u32` timestamp that corresponds to now
    let now = rtfm::now();

    let t = &amp;mut ctxt.threshold;

    println!(&quot;a(ss={}, now={})&quot;, ctxt.scheduled_start, now);

    a.tasks.a.schedule_after(t, 1 * S, ());
}
#}</code></pre></pre>
<p>This program runs a single task that's executed every second and it prints the following:</p>
<pre><code class="language-text">a(ss=0, now=71)
a(ss=8000000, now=8000171)
a(ss=16000000, now=16000171)
</code></pre>
<p><code>init</code> is not a task but all tasks scheduled from it assume that <code>init</code> has a scheduled start of <code>t = 0</code> which represents the time at which <code>init</code> ends and all tasks can start. <code>schedule_now</code> makes
the scheduled task inherit the scheduled start of the current task; in this case the first instance
of <code>a</code> inherits the scheduled start of <code>init</code>, that is <code>t = 0</code>.</p>
<p>Task <code>a</code> schedules itself to run <code>S</code> cycles (1 second) in the future. The scheduled start of
its next instance will be its current scheduled start plus <code>S</code> cycles. Thus, the second instance of
<code>a</code> is scheduled to start at <code>t = 1 * S</code>, the third instance is scheduled to start at <code>t = 2 * S</code>
and so on. Note that it doesn't matter when or where in the body of <code>a</code> <code>schedule_after</code> is invoked;
the outcome will be the same.</p>
<p>Now the <code>scheduled_start</code> of a task is not the <em>exact</em> time at which the task will run -- this can
be seen in the output of the above program: <code>now</code> doesn't match the scheduled start. There's some
overhead in the task dispatcher so a task will usually run dozens of cycles after its scheduled
start time. Also, priority based scheduling can make lower priority tasks run much later than their
scheduled start time; for example, imagine the scenario where two tasks have the same scheduled
start but different priorities.</p>
<a class="header" href="print.html#scheduled_after-and-events" id="scheduled_after-and-events"><h2><code>scheduled_after</code> and events</h2></a>
<p>Tasks that spawn from <code>init</code> have predictable scheduled starts because <code>init</code> itself has a scheduled
start of <code>t = 0</code>, but what happens with tasks triggered by events which can start at any time? These
tasks use <code>rtfm::now()</code> as an <em>estimate</em> of their scheduled start. In the best-case scenario
<code>rtfm::now()</code> will be very close to the time at which the event happened. But, if the task has low
priority it may not run until other high priority tasks are done; in this scenario <code>rtfm::now()</code>,
and thus the estimated scheduled start, could be very far off from the real time at which the event
happened. Take this in consideration when using <code>scheduled_after</code> from tasks triggered by events.</p>
<a class="header" href="print.html#under-the-hood" id="under-the-hood"><h1>Under the hood</h1></a>
<p>This section describes the implementation of RTFM. This information is useful to both users of the
Cortex-M implementation of RTFM and developers interested in porting RTFM to other architectures.
The former group will get a better understanding of the performance characteristics of RTFM; the
latter group will get a high level overview of the Cortex-M implementation that they wouldn't
otherwise get from just reading the code.</p>
<a class="header" href="print.html#the-scheduler" id="the-scheduler"><h1>The scheduler</h1></a>
<p>The RTFM framework includes a priority based scheduler. In the Cortex-M implementation of RTFM the
<a href="https://developer.arm.com/docs/ddi0337/e/nested-vectored-interrupt-controller">NVIC</a> (Nested Vector Interrupt Controller), a Cortex-M core peripheral, does the actual task
scheduling -- this greatly reduces the bookkeeping that needs to be done in software.</p>
<p>All tasks map one way or another to an interrupt. This lets the NVIC schedule tasks as if they were
interrupts. The NVIC dispatches interrupt handlers according to their priorities; this gives up
priority based scheduling of tasks for free.</p>
<p>The NVIC contains a interrupt priority registers (IPR) where the <em>static</em> priority of an interrupt
can be set. The priorities assigned to tasks by the user are programmed into these registers after
<code>init</code> ends and before <code>idle</code> starts, while the interrupts are disabled.</p>
<p>The IPR registers store priorities in a different way than the user specifies them so a conversion
is needed. To distinguish these two we refer to the IPR format as <em>hardware</em> priority level, and we
refer to the priority entered in <code>app!</code> as the <em>logical</em> priority level.</p>
<p>In hardware priority levels a bigger number indicates <em>lower</em> urgency and vice versa. Plus, Cortex-M
devices only support a certain number of priority bits: for example 4 bits equates 16 different
priority levels. These priority bits correspond to the higher bits of each 8-bit IPR register.</p>
<p>Different devices support different number of priority bits so this needs to be accounted for when
converting from a logical priority level to a hardware priority level. This is what the conversion
routine looks like:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// number of priority bits (device specific)
const NVIC_PRIO_BITS: u8 = 4;

fn logical2hardware(prio: u8) -&gt; u8 {
    ((1 &lt;&lt; NVIC_PRIO_BITS) - prio) &lt;&lt; (8 - NVIC_PRIO_BITS)
}
#}</code></pre></pre>
<p>The RTFM runtime needs to know <code>NVIC_PRIO_BITS</code> for the target device to properly configure the
priority of each task. Currently the <code>app!</code> macro expects the <code>device</code> crate to contain this
information as a <code>u8</code> constant at <code>$device::NVIC_PRIO_BITS</code>.</p>
<a class="header" href="print.html#claim-1" id="claim-1"><h1><code>claim</code></h1></a>
<p>At the center of RTFM we have the <code>Resource</code> abstraction. A <code>Resource</code> is a mechanism to share data
between two or more tasks (contexts of execution) that can potentially run at different priorities.
When tasks have different priorities they can preempt each other and this can lead to data races if
the access to the data is <em>not</em> synchronized. A <code>Resource</code> eliminates the data race problem by
forcing the tasks to access the data through a critical section. While in a critical section the
other tasks that share the <code>Resource</code> can <em>not</em> start.</p>
<p>As tasks in RTFM are all dispatched in interrupt handlers one way to create a critical section is to
disable all interrupts (<code>cpsid i</code> instruction). However, this approach also prevents tasks that are
not contending for the resource from starting, which can reduce the responsiveness of the system.
The Cortex-M implementation uses priority based critical sections (AKA Priority Ceiling Protocol) to
avoid this problem, or at least to reduce its effect.</p>
<p>The NVIC, which is the core of the RTFM scheduler, supports dynamic reprioritization of interrupts
via the <a href="https://developer.arm.com/products/architecture/m-profile/docs/100701/latest/special-purpose-mask-registers">BASEPRI</a> register. By writing to this register we can increase the priority of the current
interrupt / task preventing tasks with lower priority from starting. A temporal increase of the
priority can be used as a critical section; this is how <code>claim</code> works in the Cortex-M implementation
of RTFM.</p>
<p>The question is how much to increase the priority in these critical sections? The value must be high
enough to prevent data races but not too high that it blocks unrelated tasks. The answer to this
question comes from the Priority Ceiling Protocol: each resource has a priority <em>ceiling</em>; to access
the data a critical section must be created by temporarily increasing the priority to match the
priority ceiling; the priority ceiling of a resource is equal to the priority of the highest
priority task that can access the resource.</p>
<p>In the Cortex-M implementation of RTFM we store the ceiling of a resource in the type system and we
also track the dynamic priority of a task using the type system. The main reason for this is
generating optimal machine code for <code>claim</code>s.</p>
<p>Here's what the <code>Resource</code> abstraction looks like:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
/// Priority token
pub struct Priority&lt;P&gt; { _not_send_or_sync: *const (), _priority: PhantomData&lt;P&gt; }

pub unsafe trait Resource {
    /// The number of priority bits supported by the NVIC (device specific)
    const NVIC_PRIO_BITS: u8;

    /// The priority &quot;ceiling&quot; of this resource
    type Ceiling: Unsigned; // type level integer (cf. typenum)

    /// The data protected by this resource
    type Data: 'static + Send;

    // Returns a reference to the `static mut` variable protected by this resource
    #[doc(hidden)]
    unsafe fn _var() -&gt; &amp;'static mut Self::Data;

    /// Borrows the resource data while the priority is high enough
    // NOTE there's a mutable version of this method: `borrow_mut`
    fn borrow&lt;P, 'p&gt;(&amp;'t self, p: &amp;'p Priority&lt;P&gt;) -&gt; &amp;'p Self::Data
    where
        P: IsGreaterOrEqual&lt;Self::Ceiling, Output = True&gt;,
    {
        unsafe { Self::_var() }
    }

    /// Claim the data proceted by this resource
    // NOTE there's a mutable version of this method: `claim_mut`
    fn claim&lt;P&gt;(&amp;self, t: &amp;mut Priority&lt;P&gt;, f: F)
    where
        F: FnOnce(&amp;Self::Data, &amp;mut Priority&lt;Maximum&lt;P, Self::Ceiling&gt;)
        P: Max&lt;Self::Ceiling&gt; + Unsigned,
        Self::Ceiling: Unsigned,
    {
        unsafe {
            if P::to_u8() &gt;= Self::Ceiling::to_u8() {
                // the priority doesn't need to be raised further
                f(Self::get(), &amp;mut Priority::new())
            } else {
                // the hardware priority ceiling of this resource
                let new = (1 &lt;&lt; Self::NVIC_PRIO_BITS - Self::Ceiling::to_u8()) &lt;&lt;
                    (8 - Self::NVIC_PRIO_BITS);

                let old = basepri::read();

                // start the critical section by raising the dynamic priority
                basepri::write(new);

                // execute user provided code inside the critical section
                let r = f(Self::get(), &amp;mut Priority::new());

                // end the critical section by restoring the old dynamic priority
                basepri::write(old);

                r
            }
        }
    }
}
#}</code></pre></pre>
<p>The <code>Priority</code> <em>token</em> is used to track the current dynamic priority of a task. When a task starts
its <code>Context</code> contains a <code>Priority</code> token that represents the priority declared in <code>app!</code>. For
example, if the task priority was set to <code>2</code> the threshold token will have type <code>Threshold&lt;U2&gt;</code>
where <code>U2</code> is the type level version of <code>2</code> (cf. <a href="https://docs.rs/typenum"><code>typenum</code></a>).</p>
<p>The <code>claim</code> method creates a critical section by temporarily raising the task priority. Within this
critical section (closure) a new <code>Priority</code> token is provided while the outer <code>Priority</code> token is
invalidated due to borrow semantics (mutably borrowed / frozen).</p>
<p>When generating code the <code>app!</code> macro creates a <code>struct</code> that implements the <code>Resource</code> trait for
each resource declared in <code>resources</code>. The data behind each <code>Resource</code> is a <code>static mut</code> variable:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// given: `resources: { static FOO: u32 = 0 }`

// app! produces
mod __resource {
    pub struct FOO { _not_send_or_sync: *const () }

    unsafe impl Resource for FOO {
        const NVIC_PRIO_BITS = stm32f103xx::NVIC_PRIO_BITS;

        type Ceiling = U3;

        type Data = u32;

        unsafe fn _var() -&gt; &amp;'static mut u32 {
            static mut FOO: u32 = 0;

            &amp;mut FOO
        }
    }
}
#}</code></pre></pre>
<p>Theses resource <code>struct</code> are packed in <code>Resources</code> <code>struct</code>s and then placed in the <code>Context</code> of
each task.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// given: `tasks: { a: { resources: [FOO, BAR] } }`

// app! produces
mod a {
    pub struct Context {
        pub resources: Resources,
        // ..
    }

    pub struct Resources {
        pub FOO: __resource::FOO,
        pub BAR: __resource::BAR,
    }
}
#}</code></pre></pre>
<a class="header" href="print.html#message-passing-1" id="message-passing-1"><h1>Message passing</h1></a>
<p>This section describes how message passing is implemented in RTFM.</p>
<a class="header" href="print.html#dispatching-tasks" id="dispatching-tasks"><h1>Dispatching tasks</h1></a>
<p>Let's first analyze the simpler case of dispatching tasks with <code>input</code> type of <code>()</code>, i.e. the
message contained no payload, and was scheduled using <code>schedule_now</code>.</p>
<p>All tasks scheduled by other tasks, i.e. tasks not bound to an interrupt, that are to be executed at
the same priority are dispatched from the same <em>task dispatcher</em>. Task dispatchers are implemented
on top of the free interrupt handlers which are declared in <code>free_interrupts</code>. Each task dispatcher
has a queue of tasks ready to execute -- this queues are called <em>ready queues</em>. RTFM uses
<code>heapless::RingBuffer</code> for all the internal queues; these queues are lock-free and wait-free when
the queue has a single consumer and a single producer.</p>
<p>Let's illustrate the workings of task dispatchers with an example. Assume we have an application
with 4 tasks not bound to interrupts: two of them, <code>a</code> and <code>b</code>, are dispatched at priority 1; and
the other two, <code>c</code> and <code>d</code>, are dispatched at priority 2. This is what the task dispatchers produced
by the <code>app!</code> macro look like:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// priority = 1
unsafe extern &quot;C&quot; fn EXTI0() {
    while let Some(task) = __1::READY_QUEUE.dequeue() {
        match task {
            __1::Task::a =&gt; a(a::Context::new()),
            __1::Task::b =&gt; b(b::Context::new()),
        }
    }
}

// priority = 2
unsafe extern &quot;C&quot; fn EXTI1() {
    while let Some(task) = __2::READY_QUEUE.dequeue() {
        match task {
            __2::Task::c =&gt; c(c::Context::new()),
            __2::Task::d =&gt; d(d::Context::new()),
        }
    }
}

mod __1 {
    // Tasks dispatched at priority = 1
    enum Task { a, b }

    static mut READY_QUEUE: Queue&lt;Task, UN&gt; = Queue::new();
}

mod __2 {
    // Tasks dispatched at priority = 2
    enum Task { c, d }

    static mut READY_QUEUE: Queue&lt;Task, UN&gt; = Queue::new();
}
#}</code></pre></pre>
<p>Note that we have two queues here: one for priority = 1 and another for priority = 2. The
interrupts used to dispatch tasks are chosen from the list of <code>free_interrupts</code> declared in the
<code>app!</code> macro.</p>
<a class="header" href="print.html#payloads" id="payloads"><h4>Payloads</h4></a>
<p>Now let's add payloads to the messages. The message queues will now not only store the task name
(<code>enum Task</code>) but also an <em>index</em> (<code>u8</code>) to the payload.</p>
<p>Let's first look at how the first task dispatcher changed: let's say that tasks <code>a</code> and <code>b</code> now
expect payloads of <code>i32</code> and <code>i16</code>, respectively.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
mod a {
    static mut PAYLOADS: [i32; N] = unsafe { uninitialized() };

    static mut FREE_QUEUE: Queue&lt;u8, UN&gt; = Queue::new();

    // ..
}

mod b {
    static mut PAYLOADS: [i16; N] = unsafe { uninitialized() };

    static mut FREE_QUEUE: Queue&lt;u8, UN&gt; = Queue::new();

    // ..
}

mod __1 {
    // Tasks dispatched at priority = 1
    enum Task { a, b }

    static mut READY_QUEUE: Queue&lt;(Task, u8), UN&gt; = Queue::new();
}

mod __2 {
    // Tasks dispatched at priority = 2
    enum Task { c, d }

    static mut READY_QUEUE: Queue&lt;(Task, u8), UN&gt; = Queue::new();
}

// priority = 1
unsafe extern &quot;C&quot; fn EXTI0() {
    while let Some(task) = READY_QUEUE.dequeue() {
        match (task, index) {
            __1::Task::a =&gt; {
                let payload: i32 = ptr::read(&amp;a::PAYLOADS[index]);
                a::FREE_QUEUE.enqueue_unchecked(index);

                a(a::Context::new(payload))
            },
            __2::Task::b =&gt; {
                let payload: i16 = ptr::read(&amp;b::PAYLOADS[index]);
                b::FREE_QUEUE.enqueue_unchecked(index);

                b(b::Context::new(payload))
            },
        }
    }
}
#}</code></pre></pre>
<p>Each task dispatcher continuously dequeues tasks from the ready queue until it's empty. After
dequeuing a task - index pair the task dispatcher looks at which task it has to execute (<code>match</code>)
and uses this information to fetch (<code>ptr::read</code>) the payload from the corresponding list of
payloads (<code>PAYLOADS</code>) -- there's one such list per task. After retrieving the payload this leaves an
empty slot in the list of payloads; the index to this empty slot is appended to a list of free slots
(<code>FREE_QUEUE</code>). Finally, the task dispatcher proceed to execute the task using the message payload
as the input.</p>
<a class="header" href="print.html#schedule_now" id="schedule_now"><h1><code>schedule_now</code></h1></a>
<p>We saw how tasks dispatching works; now let's see how <code>schedule_now</code> is implemented. Assume that
task <code>a</code> can be <code>schedule_now</code>-ed by task <code>b</code>; in this scenario the <code>app!</code> macro generates code like
this:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
mod __schedule_now {
    pub struct a { _not_send_or_sync: PhantomData&lt;*const ()&gt; }

    impl a {
        fn schedule_now(&amp;mut self, t: &amp;mut Threshold, payload: i32) -&gt; Result&lt;(), i32&gt; {
            if let Some(index) = a::FREE_QUEUE.claim_mut(t, |fq, _| fq.dequeue()) {
                ptr::write(&amp;mut a::PAYLOADS[index], payload);

                __1::READY_QUEUE.claim_mut(t, |rq, _| {
                    rq.enqueue_unchecked((__1::Task::A, index))
                });

                NVIC.set_pending(Interrupt::EXTI0);
            } else {
                Err(payload)
            }
        }
    }
}

mod b {
    pub struct Tasks { a: __schedule_now::a }

    pub struct Context {
        tasks: Tasks,
        // ..
    }
}
#}</code></pre></pre>
<p>The first thing to do to schedule a new task is to get a free slot, where to store the payload, from
the <code>FREE_QUEUE</code>. If the list of payloads (<code>PAYLOADS</code>) is full, i.e. if <code>FREE_QUEUE</code> is empty, then
<code>schedule_now</code> early returns with an error. After retrieving a free slot the <code>payload</code> is stored
into it. Then the task - index pair is enqueued into the corresponding priority queue. Finally, the
interrupt whose handler is being used as task dispatcher is set as <em>pending</em> -- this will cause the
<code>NVIC</code> (the hardware scheduler) to execute the handler.</p>
<p>Fetching a free slot from the free queue and enqueuing a task - index pair into the ready queue may
require critical sections so the queues are accessed as resources using <code>claim_mut</code>. In a later
section we'll analyze where critical sections are required.</p>
<a class="header" href="print.html#capacity" id="capacity"><h1>Capacity</h1></a>
<p>All the queues and arrays internally used by the RTFM runtime are fixed in size and allocated in
<code>static</code> variables. The user directly controls the size of most of these data structures via the
<code>capacity</code> property of a task in the <code>app!</code> specification.</p>
<p>For example if the user specifies that task <code>a</code> has a <code>capacity</code> of 3 then the <code>app!</code> macro
generates the following code:</p>
<pre><pre class="playpen"><code class="language-rust">mod a {
    const CAPACITY: usize = 3;

    static mut FREE_QUEUE: Queue&lt;u8, U3&gt; = Queue::new();

    static mut PAYLOADS: [i32; CAPACITY] = unsafe { uninitialized() };
}

// generated by `app!`
fn main() {
    unsafe {
        // ..

        // initialize the `FREE_QUEUE` of each task
        for index in 0..a::CAPACITY {
            a::FREE_QUEUE.enqueue_unchecked(index as u8);
        }

        // ..

        // call user provided `init`
        init(init::Context());

        // ..
    }
}
</code></pre></pre>
<p>There is a choice to be made by the implementers of the runtime when it comes to the size (capacity)
of the ready queues.</p>
<p>Ready queues hold instances of tasks pending execution of potentially different types. However, for
each task we know the maximum number of instances that can be scheduled and pending execution; this
information is in the specification (<code>capacity</code>). If we choose the capacity of the ready queue to be
the sum of the max number of instances of each different task it can hold then we can eliminate the
possibility of it ever running out of capacity -- in the worst case scenario the ready queue will
become full. In the Cortex-M implementation of RTFM we chose this sum as the capacity of the ready
queues; this let us eliminate capacity checks when adding new tasks to the ready queue.</p>
<a class="header" href="print.html#the-timer-queue" id="the-timer-queue"><h1>The timer queue</h1></a>
<p>In this section we explore the <em>timer queue</em>, the backbone of the <code>scheduled_in</code> API.</p>
<p>The <code>schedule_in</code> method schedules a task run in the future. <code>schedule_in</code> doesn't directly enqueue
tasks into the ready queues, instead it enqueues them in the <em>timer queue</em>. The timer queue is a
priority queue that prioritizes tasks with the nearest scheduled start. Associated to the timer
queue there is an interrupt handler that moves tasks that have become ready into the ready queues.</p>
<a class="header" href="print.html#schedule_after" id="schedule_after"><h1><code>schedule_after</code></h1></a>
<p>Let's see how <code>schedule_after</code> adds tasks to the timer queue.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
mod __schedule_after {
    impl a {
        fn schedule_after(
            &amp;mut self,
            t: &amp;mut Threshold,
            offset: u32,
            payload: i32,
        ) -&gt; Result&lt;(), i32&gt; {
            if let Some(index) = a::FREE_QUEUE.dequeue() {
                core::ptr::write(
                    &amp;mut a::PAYLOADS[index as usize],
                    payload,
                );

                let scheduled_start = self.scheduled_start + offset;

                core::ptr::write(
                    &amp;mut a::SCHEDULED_STARTS[index as usize],
                    scheduled_start,
                );

                let not_ready = NotReady {
                    index,
                    scheduled_start,
                    task: __tq::Task::a,
                };

                __tq::TIMER_QUEUE.claim_mut(t, |tq, _| tq.enqueue(not_ready));
            } else {
                Err(payload)
            }
        }
    }
}
#}</code></pre></pre>
<p>Like <code>schedule_now</code>, <code>schedule_after</code> starts by fetching a free slot from the <code>FREE_QUEUE</code>. If
there's no free slot available the function early returns with an error. Once a free slot (<code>index</code>)
has been retrieved the payload is stored in that spot of the payload list (<code>PAYLOADS</code>). The
<code>scheduled_start</code> of the newly scheduled task is the <code>scheduled_start</code> time of the current task plus
the specified <code>offset</code>. This <code>scheduled_start</code> value is also stored in a list (<code>SCHEDULED_STARTS</code>)
at the free slot <code>index</code>.  After that's done, the not ready task -- represented by the <code>NotReady</code>
struct which contains the <code>Task</code> name, the payload / <code>scheduled_after</code> index and the actual
<code>scheduled_start</code> value -- is inserted in the timer queue.</p>
<p><code>TimerQueue.enqueue</code> does a bit more of work than just adding the not ready task to the priority
queue of tasks:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
struct TimerQueue {
    priority_queue: BinaryHeap&lt;..&gt;,
}

impl TimerQueue {
    unsafe fn enqueue(&amp;mut self, new: NotReady) {
        let mut is_empty = true;

        if self.priority_queue
            .peek()
            .map(|head| {
                is_empty = false;
                new.scheduled_start &lt; head.scheduled_start
            })
            .unwrap_or(true)
        {
            if is_empty {
                SYST.enable_interrupt();
            }

            SCB.set_pending(Exception::SysTick);
        }

        self.priority_queue.push_unchecked(new);
    }
}
#}</code></pre></pre>
<p>If the priority queue is empty or the new not ready task is scheduled to run <em>before</em> the current
task at the front of the queue then the <code>SysTick</code> exception handler is also enabled and set as
pending. In the next section we'll see the role that this handler plays.</p>
<p>Another important thing to note is that the <code>Task</code> enum used in the <code>NotReady</code> struct: it only
contains tasks which can be scheduled via <code>scheduled_after</code>. The tasks in this set not necessarily
are to be dispatched at the same priority.</p>
<p>Consider the following task configuration:</p>
<ul>
<li>Tasks <code>a</code> and <code>b</code> are dispatched at priority 1</li>
<li>Tasks <code>c</code> and <code>d</code> are dispatched at priority 2</li>
<li><code>a</code> is scheduled using <code>schedule_after</code></li>
<li><code>b</code> is scheduled using <code>schedule_now</code></li>
<li><code>c</code> is scheduled using <code>schedule_now</code></li>
<li><code>d</code> is scheduled both via <code>schedule_now</code> and <code>scheduled_after</code></li>
</ul>
<p>RTFM will end up creating the following <code>enum</code>s:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
mod __1 {
    enum Task { a, b }
}

mod __2 {
    enum Task { c, d }
}

mod __tq {
    enum Task { a, d }
}
#}</code></pre></pre>
<a class="header" href="print.html#the-timer-queue-handler" id="the-timer-queue-handler"><h1>The timer queue handler</h1></a>
<p>The <code>SysTick</code> exception handler is used as the timer queue handler. This handler takes cares of
moving tasks that have become ready from the timer queue to their respective ready queues. The
timer queue makes use of the Cortex-M sytem timer, the <code>SysTick</code>, to schedule when the <code>SysTick</code>
handler should run.</p>
<p>This is what the <code>SYS_TICK</code> handler looks like for our running example where <code>a</code> and <code>d</code> are
scheduled via <code>scheduled_after</code>:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
unsafe extern &quot;C&quot; fn SYS_TICK() {
    let mut t = Threshold::new(..);

    loop {
        let next = TQ.claim_mut(&amp;mut t, |tq, _| {
            let front = tq.priority_queue.peek().map(|nr| nr.scheduled_start);

            if let Some(scheduled_start) = front {
                let diff = scheduled_start - Instant::now();

                if diff &gt; 0 {
                    // task still not ready, schedule this handler to run in the future by
                    // setting a new timeout

                    // maximum timeout supported by the SysTick
                    const MAX: u32 = 0x00ffffff;

                    SYST.set_reload(cmp::min(MAX, diff as u32));

                    // start counting from the new reload
                    SYST.clear_current();

                    None
                } else {
                    // task became ready
                    let nr = tq.priority_queue.pop_unchecked();

                    Some((nr.task, nr.index))
                }
            } else {
                // the queue is empty
                SYST.disable_interrupt();

                None
            }
        });

        if let Some((task, index)) = next {
            // place the tasks - index pair into the corresponding ready queue
            match task {
                __tq::Task::a =&gt; {
                    __1::READY_QUEUE.claim_mut(t, |rq, _| {
                        rq.enqueue_unchecked((__1::Task::a, index));
                    });

                    NVIC.set_pending(Interrupt::EXTI0);
                },
                __tq::Task::d =&gt; {
                    __2::READY_QUEUE.claim_mut(t, |rq, _| {
                        rq.enqueue_unchecked((__2::Task::d, index));
                    });

                    NVIC.set_pending(Interrupt::EXTI1);
                },
            }
        } else {
            return;
        }
    }
}
#}</code></pre></pre>
<p>The <code>SYS_TICK</code> handler will use a <code>loop</code> to move all the tasks that have become ready from the
priority queue, the timer queue, to the ready queues.</p>
<p>To do that the handler will check the front of the priority queue, which contains the task with the
closest <code>scheduled_start</code>. If the queue is empty then the handler will disable the <code>SysTick</code>
exception and return; the handler won't run again until the exception is re-enabled by
<code>TimerQueue.enqueue</code>.</p>
<p>If the priority queue was not empty then the handler will then compare that closest
<code>scheduled_start</code> against the current time (<code>Instant::now()</code>). If the <code>scheduled_start</code> time has not
been reached the handler will schedule to run itself in the future by setting a <code>SysTick</code> timeout.
If instead we are past the closest <code>scheduled_start</code> then the handler will move the task at the
front of the queue to its corresponding <code>READY_QUEUE</code> and set the corresponding task dispatcher as
pending.</p>
<a class="header" href="print.html#ceiling-analysis" id="ceiling-analysis"><h1>Ceiling analysis</h1></a>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        <script type="text/javascript">
            document.addEventListener('DOMContentLoaded', function() {
                window.print();
            })
        </script>
        

        

        
        <script src="searchindex.js" type="text/javascript" charset="utf-8"></script>
        
        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

    </body>
</html>
